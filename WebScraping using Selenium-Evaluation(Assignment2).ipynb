{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7dcf92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium \n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from selenium.webdriver import ActionChains, Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fea3608",
   "metadata": {},
   "source": [
    "1. In this question you have to scrape data using the filters available on the webpage You have to use the location and\n",
    "salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bafcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the naukri page on automated chrome browser\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\" https://www.naukri.com/\")\n",
    "#maximize chrome window\n",
    "driver.maximize_window()\n",
    "\n",
    "#entering designation as required in question\n",
    "designation= driver.find_element(By.CLASS_NAME, \"suggestor-input \")\n",
    "designation.send_keys('Data Scientist')\n",
    "\n",
    "#After filling designation, clicking on the search button\n",
    "Search= driver.find_element(By.CLASS_NAME,\"qsbSubmit\")\n",
    "Search.click()\n",
    "time.sleep(3)\n",
    "\n",
    "#Selecting All Filter element\n",
    "All_filter = driver.find_element(By.CLASS_NAME, 'styles_filter-wrapper-component__4OBpS')\n",
    "All_filter.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Selecting \"Delhi / NCR\" as the job location\n",
    "specific_location = driver.find_element(By.XPATH, '//label[@for=\"chk-Delhi / NCR-cityTypeGid-\"]')\n",
    "specific_location.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Selecting the salary range of \"3-6 Lakhs\"\n",
    "Salary= driver.find_element(By.XPATH, '//label[@for=\"chk-3-6 Lakhs-ctcFilter-\"]')\n",
    "Salary.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Initializing empty lists to store scraped data\n",
    "titles=[]\n",
    "Company_names=[]\n",
    "Experience_required=[]\n",
    "Location=[]\n",
    "\n",
    "# Scraping job titles\n",
    "job_title = driver.find_elements(By.XPATH, '//div[@class=\"cust-job-tuple layout-wrapper lay-2 sjw__tuple \"]/div/a')\n",
    "for i in job_title[:10] :\n",
    "    title=i.text\n",
    "    titles.append(title)\n",
    "   \n",
    " # Scraping company names\n",
    "for i in range(1,20):\n",
    "    try:\n",
    "        company_tag = driver.find_element(By.XPATH, '//*[@id=\"listContainer\"]/div[2]/div/div[' + str(i) + ']/div/div[2]/span/a[1]').text\n",
    "        Company_names.append(company_tag)\n",
    "        if len(Company_names) == 10:\n",
    "            break\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Scraping job locations\n",
    "Location_tag = driver.find_elements(By.XPATH, '//span[@class=\"locWdth\"]')\n",
    "for i in Location_tag[:10]:\n",
    "    Location_name=i.text\n",
    "    Location.append(Location_name)\n",
    "    \n",
    "\n",
    "# Scraping experience required\n",
    "Experience_tag = driver.find_elements(By.XPATH, '//span[@class=\"expwdth\"]')\n",
    "for i in Experience_tag[:10]:\n",
    "    Experience=i.text\n",
    "    Experience_required.append(Experience)\n",
    "  \n",
    "\n",
    "    \n",
    "# Creating a DataFrame from the scraped data   \n",
    "df = pd.DataFrame({'Job Title':titles, 'Company Name':Company_names,'Experience Required':Experience_required,'Job Location':Location,})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb48ef2",
   "metadata": {},
   "source": [
    "2. Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the\n",
    "job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a460ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the shine page on automated chrome browser\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.shine.com/\")\n",
    "#maximize chrome window\n",
    "driver.maximize_window()\n",
    "\n",
    "# Find the 'Jobs' element\n",
    "we = driver.find_element(By.XPATH,\"/html/body/div/header/div[5]/div/ul/li[2]/a\")\n",
    "time.sleep(3)\n",
    "\n",
    "# Create an ActionChains object\n",
    "actions = ActionChains(driver)\n",
    "\n",
    "# Hover over the 'Jobs' element\n",
    "actions.move_to_element(we).perform()\n",
    "time.sleep(3)\n",
    "\n",
    "# Find and click the 'Search' button after hovering over 'Jobs'\n",
    "job_search = driver.find_element(By.XPATH, \"/html/body/div/header/div[5]/div/ul/li[2]/ul/li[1]/a\")\n",
    "job_search.click()\n",
    "\n",
    "#entering job position and location as required in question\n",
    "driver.find_element(By.XPATH,'//*[@id=\"id_q\"]').send_keys(\"Data Analyst\")\n",
    "driver.find_element(By.XPATH,'//*[@id=\"id_loc\"]').send_keys(\"Bangalore\")\n",
    "time.sleep(3)\n",
    "#After filling details, clicking on the search button\n",
    "driver.find_element(By.XPATH, '/html/body/div/div[2]/div/div/div[1]/p').click()\n",
    "driver.find_element(By.XPATH, '//*[@id=\"__next\"]/div[2]/div/div/div[1]/div[1]/div/div[2]/div/div[1]/form/div/div[2]/div/button').click()\n",
    "\n",
    "# Initializing empty lists to store scraped data\n",
    "titles=[]\n",
    "Company_names=[]\n",
    "Experience_required=[]\n",
    "Location=[]\n",
    "\n",
    "# Scraping job data for the first 10 jobs\n",
    "for i in range(1,11):\n",
    "    # Finding elements by XPath for job title, company name, experience required, and job location\n",
    "    job_title = driver.find_element(By.XPATH, '//*[@id=\"1\"]/div[' + str(i) +']/div[1]/div[1]/strong/h2/a').text\n",
    "    company_name = driver.find_element(By.XPATH, '//*[@id=\"1\"]/div[' + str(i) + ']/div[1]/div[1]/div[2]/span').text\n",
    "    experience_required = driver.find_element(By.XPATH, '//*[@id=\"1\"]/div[' + str(i) + ']/div[1]/div[1]/div[3]/div[1]').text\n",
    "    job_location = driver.find_element(By.XPATH, '//*[@id=\"1\"]/div[' + str(i) + ']/div[1]/div[1]/div[3]/div[2]').text\n",
    "    \n",
    "    # Appending scraped data to respective lists\n",
    "    titles.append(job_title)\n",
    "    Company_names.append(company_name)\n",
    "    Experience_required.append(experience_required)\n",
    "    # Splitting the location string to get only the city name\n",
    "    Location_job = job_location.split('\\n')\n",
    "    Location.append(Location_job[0])\n",
    "\n",
    "# Creating a DataFrame from the scraped data    \n",
    "df = pd.DataFrame({'Job Title':titles,'Company Name':Company_names,'Experience Required':Experience_required,'Job Location':Location})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891246dd",
   "metadata": {},
   "source": [
    "3.Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7108096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening flipkart.com on automated chrome browser\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\")\n",
    "#maximize chrome window\n",
    "driver.maximize_window()\n",
    "\n",
    "# Initializing empty lists to store scraped data\n",
    "Ratings=[]\n",
    "Review_Summery=[]\n",
    "Full_Review=[]\n",
    "\n",
    "# Define start and end page numbers for scraping\n",
    "start=0\n",
    "end=10\n",
    "\n",
    "# Set the maximum number of products to scrape\n",
    "max_reviews = 100\n",
    "\n",
    "# Variables to track the total number of products scraped for each category\n",
    "total_ratings = 0\n",
    "total_review_summery = 0\n",
    "total_full_review = 0\n",
    "\n",
    "# Loop through each page of reviews\n",
    "for page in range(start,end):\n",
    "    # Scraping ratings\n",
    "    Rating_tag = driver.find_elements(By.XPATH, '//div[@class=\"XQDdHH Ga3i8K\"]')\n",
    "    for i in Rating_tag[:10] :\n",
    "        Rating=i.text\n",
    "        Ratings.append(Rating)\n",
    "        total_ratings += 1\n",
    "        # Checking if the maximum number of products has been reached\n",
    "        if total_ratings >= max_reviews:\n",
    "            break\n",
    "    \n",
    "    # Scraping review summaries\n",
    "    Review_tag= driver.find_elements(By.CLASS_NAME, \"z9E0IG\")\n",
    "    for i in Review_tag:\n",
    "        Review=i.text\n",
    "        Review_Summery.append(Review)\n",
    "        total_review_summery += 1\n",
    "        if total_review_summery >= max_reviews:\n",
    "            break\n",
    "    \n",
    "     # Scraping full reviews\n",
    "    Full_tag= driver.find_elements(By.CLASS_NAME, 'ZmyHeo')\n",
    "    for i in Full_tag:\n",
    "        Full=i.text\n",
    "        Full_Review.append(Full)\n",
    "        total_full_review += 1\n",
    "        if total_full_review >= max_reviews:\n",
    "            break\n",
    "    \n",
    "    # Clicking on the next page button\n",
    "    if page == 0:\n",
    "        next_button= driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]').click()\n",
    "        time.sleep(3)\n",
    "    elif page <=8 :\n",
    "        next_button= driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"][2]').click()\n",
    "        time.sleep(3)\n",
    "    \n",
    "\n",
    "# Creating a DataFrame from the scraped data\n",
    "df = pd.DataFrame({\"Ratings\":Ratings,\"Review Summery\" : Review_Summery,\"Full Review\":Full_Review})\n",
    "df\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefa50b3",
   "metadata": {},
   "source": [
    "4.: Scrape data forfirst 100 sneakers you find whenyouvisitflipkart.com and search for “sneakers” inthe search\n",
    "field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. ProductDescription\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055d5c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the flipkart page on automated chrome browser\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\" https://www.flipkart.com/\")\n",
    "#maximize chrome window\n",
    "driver.maximize_window()\n",
    "\n",
    "#finding the element and Simulating typing \"sneakers\" into the found element\n",
    "product= driver.find_element(By.CLASS_NAME, \"Pke_EE\")\n",
    "product.send_keys('sneakers')\n",
    "\n",
    "#finding the search button and Clicking on it\n",
    "Search= driver.find_element(By.XPATH,\"/html/body/div[1]/div/div[1]/div/div/div/div/div[1]/div/div[1]/div/div[1]/div[1]/header/div[1]/div[2]/form/div/button\")\n",
    "Search.click()\n",
    "\n",
    "# Initializing empty lists to store scraped data\n",
    "Brand=[]\n",
    "ProductDescription=[]\n",
    "price=[]\n",
    "\n",
    "# Define start and end page numbers for scraping\n",
    "start=0\n",
    "end=3\n",
    "\n",
    "# Set the maximum number of products to scrape\n",
    "max_product = 100\n",
    "\n",
    "# Variables to track the total number of products scraped for each category\n",
    "total_products_brand = 0\n",
    "total_products_PD = 0\n",
    "total_products_PRICE = 0\n",
    "\n",
    "# Loop through each page in the specified range\n",
    "for page in range(start,end):\n",
    "    # Scraping brand names\n",
    "    Brand_tag= driver.find_elements(By.CLASS_NAME, \"syl9yP \")\n",
    "    for i in Brand_tag:\n",
    "        Brand_name=i.text\n",
    "        Brand.append(Brand_name)\n",
    "        total_products_brand += 1\n",
    "        # Checking if the maximum number of products has been reached\n",
    "        if total_products_brand >= max_product:\n",
    "            break\n",
    "    \n",
    "    # Scraping product descriptions\n",
    "    Product_Description= driver.find_elements(By.CLASS_NAME, \"WKTcLC\")\n",
    "    for i in Product_Description:\n",
    "        Product_Des=i.text\n",
    "        ProductDescription.append(Product_Des)\n",
    "        total_products_PD += 1\n",
    "        if total_products_PD >= max_product:\n",
    "            break\n",
    "    \n",
    "     # Scraping product prices\n",
    "    Price_tag= driver.find_elements(By.XPATH, '//div[@class=\"Nx9bqj\"]')\n",
    "    for i in Price_tag:\n",
    "        Product_Price=i.text\n",
    "        price.append(Product_Price)\n",
    "        total_products_PRICE += 1\n",
    "        if total_products_PRICE >= max_product:\n",
    "            break\n",
    "\n",
    "    \n",
    "    # Clicking the next page button based on the current page number\n",
    "    if page == 0:\n",
    "        next_button= driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"]').click()\n",
    "        time.sleep(3)\n",
    "    elif page <=1 :\n",
    "        next_button= driver.find_element(By.XPATH, '//a[@class=\"_9QVEpD\"][2]').click()\n",
    "        time.sleep(3)\n",
    "    \n",
    "\n",
    "# Creating a DataFrame from the scraped data\n",
    "df = pd.DataFrame({\"Brand\":Brand,\"Product Description\" : ProductDescription,\"Price\":price})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e005ef",
   "metadata": {},
   "source": [
    "5 Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then set CPU\n",
    "Type filter to “Intel Core i7” as shown in the below image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406c306f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the flipkart page on automated chrome browser\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.amazon.in/\")\n",
    "#maximize chrome window\n",
    "driver.maximize_window()\n",
    "time.sleep(20)\n",
    "\n",
    "#type manually if \"Captcha\"is there\n",
    "\n",
    "#finding the element and Simulating typing \"Laptop\" into the found element\n",
    "product= driver.find_element(By.XPATH, \"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[2]/div[1]/input\")\n",
    "product.send_keys('Laptop')\n",
    "\n",
    "\n",
    "#finding the search button and Clicking on it\n",
    "Search= driver.find_element(By.XPATH,\"/html/body/div[1]/header/div/div[1]/div[2]/div/form/div[3]/div\")\n",
    "Search.click()\n",
    "time.sleep(10)\n",
    "\n",
    "#Selecting All Filters element\n",
    "All_filter = driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div[2]/div/div[3]/span/div[1]/div/div/div[5]\")\n",
    "All_filter.click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Selecting the specific filter option (e.g., Intel Core i7)\n",
    "specific_CPU = driver.find_element(By.XPATH, '//*[@aria-label=\"Intel Core i7\"]/span/a/span')\n",
    "specific_CPU.click()\n",
    "time.sleep(3)\n",
    "\n",
    "# Initializing empty lists to store scraped data\n",
    "titles=[]\n",
    "Ratings=[]\n",
    "Price=[]\n",
    "\n",
    "# Scraping product price\n",
    "price_tag = driver.find_elements(By.XPATH,'//a[@class=\"a-link-normal s-no-hover s-underline-text s-underline-link-text s-link-style a-text-normal\"]/span')\n",
    "\n",
    "for price in price_tag[:10]:\n",
    "    product_price = price.text\n",
    "    Price.append(product_price)\n",
    "\n",
    "for i in range(1,11):\n",
    "    # Scraping product title\n",
    "    laptop_title = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[1]/div[1]/div/span[1]/div[1]/div[' + str(i+2) + ']/div/div/span/div/div/div/div[2]/div/div/div[1]/h2').text\n",
    "    # Scraping product rating\n",
    "    ratings = driver.find_element(By.XPATH, '//*[@id=\"search\"]/div[1]/div[1]/div/span[1]/div[1]/div[' + str(i+2) + ']/div/div/span/div/div/div/div[2]/div/div/div[2]/div[1]').text\n",
    "\n",
    "    # Appending scraped data to respective lists\n",
    "    titles.append(laptop_title)\n",
    "    Ratings.append(ratings)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Creating a DataFrame from the scraped data   \n",
    "df = pd.DataFrame({'laptop title':titles, 'Ratings':Ratings,'Price':Price})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfaf771",
   "metadata": {},
   "source": [
    "6.Write a python program to scrape data for Top 1000 Quotes of All Time.\n",
    "The above task will be done in following steps:\n",
    "1. First get the webpagehttps://www.azquotes.com/\n",
    "2. Click on TopQuote\n",
    "3. Than scrap a)Quote b) Author c) Type Of Quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a0f1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the Quotes page on automated chrome browser\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.azquotes.com/\")\n",
    "#maximize chrome window\n",
    "driver.maximize_window()\n",
    "\n",
    "# Clicking on the \"Top Quote\" option\n",
    "TopQuote= driver.find_element(By.XPATH,\"/html/body/div[1]/div[1]/div[1]/div/div[3]/ul/li[5]/a\")\n",
    "TopQuote.click()\n",
    "\n",
    "# Initializing empty lists to store scraped data\n",
    "Quotes=[]\n",
    "Authors=[]\n",
    "Types_of_Author=[]\n",
    "\n",
    "# Define start and end page numbers for scraping\n",
    "start=0\n",
    "end=10\n",
    "\n",
    "# Set the maximum number of quotes to scrape\n",
    "max_Quote= 1000\n",
    "\n",
    "# Variables to track the total number of quotes scraped for each category\n",
    "total_Quotes= 0\n",
    "total_Authors = 0\n",
    "total_Types_of_Author = 0\n",
    "\n",
    "# Loop through each page in the specified range\n",
    "for page in range(start,end):\n",
    "    # Scraping quotes\n",
    "    if page == 0:\n",
    "        pass\n",
    "    else:\n",
    "        next_button= driver.find_element(By.XPATH, '//li[@class=\"next\"]').click()\n",
    "        time.sleep(3)\n",
    "    Quote_tag= driver.find_elements(By.XPATH, '//a[@class=\"title\"]')\n",
    "    for i in Quote_tag:\n",
    "        Quote=i.text\n",
    "        Quotes.append(Quote)\n",
    "        total_Quotes += 1\n",
    "        # Checking if the maximum number of products has been reached\n",
    "        if total_Quotes >= max_Quote:\n",
    "            break\n",
    "       \n",
    "    # Scraping authors\n",
    "    Author_name= driver.find_elements(By.XPATH, '//div[@class=\"author\"]')\n",
    "    for i in Author_name:\n",
    "        Author=i.text\n",
    "        Authors.append(Author)\n",
    "        total_Authors += 1\n",
    "        if total_Authors >= max_Quote:\n",
    "            break\n",
    "      \n",
    "    # Scraping types of authors\n",
    "    Types_of_Author_tag= driver.find_elements(By.XPATH, '//div[@class=\"tags\"]')\n",
    "    for i in Types_of_Author_tag:\n",
    "        Author_type=i.text\n",
    "        Types_of_Author.append(Author_type)\n",
    "        total_Types_of_Author += 1\n",
    "        if total_Types_of_Author >= max_Quote:\n",
    "            break\n",
    "            \n",
    "\n",
    "# Creating a DataFrame from the scraped data\n",
    "df = pd.DataFrame({\"Quotes\":Quotes,\"Authors\" : Authors,\"Types of Author\":Types_of_Author})\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ca34ed",
   "metadata": {},
   "source": [
    "7.Write a python program to display list of respected former Prime Ministers of India (i.e. Name,\n",
    "Born-Dead, Term of office, Remarks) from https://www.jagranjosh.com/general-knowledge/list-of\u0002all-prime-ministers-of-india-1473165149-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7606ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opening the given url on an automated chrome browser\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.jagranjosh.com/general-knowledge/list-of\u0002all-prime-ministers-of-india-1473165149-1#itemdiv\")\n",
    "#maximize chrome window\n",
    "driver.maximize_window()\n",
    "time.sleep(10)\n",
    "\n",
    "# Try to find the table element\n",
    "try: \n",
    "    table_element=driver.find_element(By.XPATH,'/html/body/div[1]/main/div[1]/div[1]/article/div[3]/div[6]/div/table')\n",
    "    correct_index ='6'\n",
    "    time.sleep(3)\n",
    "except:\n",
    "    table_element=driver.find_element(By.XPATH,'/html/body/div[1]/main/div[1]/div[1]/article/div[3]/div[7]/div/table')\n",
    "    correct_index ='7'\n",
    "    time.sleep(3)\n",
    "    \n",
    "\n",
    "\n",
    "# Initializing empty lists to store scraped data\n",
    "PM_Name=[]\n",
    "Born_Dead=[]\n",
    "Term_of_office=[]\n",
    "Remark=[]\n",
    "\n",
    "# Scraping data from the table\n",
    "for i in range(1,20):\n",
    "    # Scraping PM Name\n",
    "    Name_tag = driver.find_element(By.XPATH, '//*[@id=\"itemdiv\"]/div[3]/div[' + correct_index + ']/div/table/tbody/tr[' + str(i+1) + ']/td[2]/div').text\n",
    "    \n",
    "    # Scraping Born and Dead years\n",
    "    Born_Dead_year = driver.find_element(By.XPATH, '//*[@id=\"itemdiv\"]/div[3]/div[' + correct_index + ']/div/table/tbody/tr[' + str(i+1) + ']/td[3]/div').text\n",
    "    \n",
    "    # Scraping Terms of Office years\n",
    "    Term_of_office_yr = driver.find_element(By.XPATH, '//*[@id=\"itemdiv\"]/div[3]/div[' + correct_index + ']/div/table/tbody/tr[' + str(i+1) + ']/td[4]').text\n",
    "    \n",
    "    # Scraping Remark\n",
    "    Remark_tag = driver.find_element(By.XPATH, '//*[@id=\"itemdiv\"]/div[3]/div[' + correct_index + ']/div/table/tbody/tr[' + str(i+1) + ']/td[5]/div').text\n",
    "   \n",
    "    # Appending scraped data to respective lists\n",
    "    PM_Name.append(Name_tag)\n",
    "    Born_Dead.append(Born_Dead_year)\n",
    "    Term_of_office.append(Term_of_office_yr)\n",
    "    Remark.append(Remark_tag)\n",
    "    \n",
    "# Creating a DataFrame from the scraped data   \n",
    "df = pd.DataFrame({'PM Name':PM_Name,'Born_Dead':Born_Dead,'Term of office':Term_of_office,'Remark':Remark})\n",
    "df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44c954",
   "metadata": {},
   "source": [
    "8.Write a python program to display list of 50 Most expensive cars in the world\n",
    "(i.e. Car name and Price) from https://www.motor1.com/\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.motor1.com/\n",
    "2. Then You have to type in the search bar ’50 most expensive cars’\n",
    "3. Then click on 50 most expensive carsin the world..\n",
    "4. Then scrap thementioned data and make the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f73b2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening the motor1.com website on an automated Chrome browser\n",
    "driver=webdriver.Chrome()\n",
    "driver.get(\"https://www.motor1.com/\")\n",
    "#maximize chrome window\n",
    "driver.maximize_window()\n",
    "time.sleep(10)\n",
    "\n",
    "# Entering the 50 most expensive cars\n",
    "Cars= driver.find_element(By.XPATH, '//input[@class=\"m1-search-panel-input m1-search-form-text\"]')\n",
    "Cars.send_keys('50 Most expensive cars in the world')\n",
    "\n",
    "# Clicking on the search button\n",
    "Search= driver.find_element(By.XPATH,'//*[@id=\"header_search_form\"]/button[1]')\n",
    "Search.click()\n",
    "\n",
    "# Clicking on the article link for the list of expensive cars\n",
    "Expensive_Cars= driver.find_element(By.XPATH, '//*[@id=\"page_index_articles_search\"]/div[9]/div/div[1]/div/div/div[1]/div/div[1]/h3/a')\n",
    "Expensive_Cars.click()\n",
    "\n",
    "# Initializing empty lists to store scraped data\n",
    "Name=[]\n",
    "Price=[]\n",
    "\n",
    "# Scraping the names and prices of the cars\n",
    "for car_index in range(1,51):\n",
    "    # Scraping car name\n",
    "    cars_name=driver.find_element(By.XPATH,'//*[@id=\"article_box\"]/div[1]/div[2]/div[2]/h3['+str(car_index) +']').text\n",
    "    # Scraping car price\n",
    "    cars_price=driver.find_element(By.XPATH,'//*[@id=\"article_box\"]/div[1]/div[2]/div[2]/p[' +str(car_index*2+2) + ']/strong').text\n",
    "    \n",
    "    # Check if the price text contains a colon (\":\")\n",
    "    if \":\" in cars_price:\n",
    "        # If there is a colon, split the string at the colon and take the second part\n",
    "        Car_Prices = cars_price.split(\":\")\n",
    "        # Strip any leading or trailing whitespace from the second part and append to the Price list\n",
    "        Price.append(Car_Prices[1].strip())\n",
    "    else:\n",
    "        # If there is no colon, directly append the entire price text to the Price list\n",
    "        Price.append(cars_price)\n",
    "    \n",
    "    # Appending the car name to the car_names list\n",
    "    Name.append(cars_name)\n",
    "    \n",
    "\n",
    " \n",
    " # Creating a DataFrame from the scraped data   \n",
    "df = pd.DataFrame({'Car Name':Name,'Car Price':Price})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ec852c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
